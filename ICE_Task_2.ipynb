{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMahabeer/ICE-Task-2/blob/main/ICE_Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sachin Mahabeer (st10053116) PDAN8412 ICE Task 2"
      ],
      "metadata": {
        "id": "8skCXWVNoVJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below was taken/adapted from multiple GeeksforGeeks sources found in the reference list and enhanced by Claude (2025)"
      ],
      "metadata": {
        "id": "Eqtgj5zCogw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M7iwTzAR-by8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, count, desc, split, explode\n",
        "import findspark\n",
        "from pyspark.sql.functions import regexp_extract, trim\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "P6jFp0Sq_oWp",
        "outputId": "8bca66b7-8405-434d-9cf0-d7f4673e5e86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b9a5b8a9f40>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3dc343db0d74:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>CNN</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "findspark.init()\n",
        "spark = SparkSession.builder.appName(\"CNN\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "base_dir = '/content/'\n",
        "train_dir = os.path.join(base_dir, 'seg_train/seg_train')\n",
        "test_dir = os.path.join(base_dir, 'seg_test/seg_test')\n",
        "\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "print(\"Class Names:\", class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "880gQQ5Pkq-_",
        "outputId": "fe3be5b2-0729-48a6-fcb2-d18ce4163e79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14034 images belonging to 6 classes.\n",
            "Found 3000 images belonging to 6 classes.\n",
            "Class Names: ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "s7Aire4bGoVb",
        "outputId": "30d08f26-2f77-4bc9-b7c8-729e8750332c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87616\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │       \u001b[38;5;34m525,702\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87616</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,702</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m526,150\u001b[0m (2.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,150</span> (2.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m526,150\u001b[0m (2.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,150</span> (2.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 402ms/step - accuracy: 0.5447 - loss: 2.0216 - val_accuracy: 0.6837 - val_loss: 0.8909\n",
            "Epoch 2/3\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 361ms/step - accuracy: 0.7765 - loss: 0.6527 - val_accuracy: 0.6947 - val_loss: 0.8828\n",
            "Epoch 3/3\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 355ms/step - accuracy: 0.8628 - loss: 0.4206 - val_accuracy: 0.7063 - val_loss: 0.8545\n"
          ]
        }
      ],
      "source": [
        "underfit_model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "underfit_model.compile(optimizer='adam',\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "underfit_model.summary()\n",
        "\n",
        "history_underfit = underfit_model.fit(\n",
        "    train_generator,\n",
        "    epochs=3,\n",
        "    validation_data=test_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "overfit_model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "overfit_model.summary()\n",
        "\n",
        "history_overfit = overfit_model.fit(\n",
        "    train_generator,\n",
        "    epochs=25,\n",
        "    validation_data=test_generator\n",
        ")"
      ],
      "metadata": {
        "id": "xL3kDVDek9IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "final_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "final_model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history_final = final_model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "brW-s-Q_lDls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = final_model.predict(test_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Final Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I2okNjgolOFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histories, titles):\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    for i, (history, title) in enumerate(zip(histories, titles)):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title(f'{title} - Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    for i, (history, title) in enumerate(zip(histories, titles)):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{title} - Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(\n",
        "    [history_underfit, history_overfit, history_final],\n",
        "    ['Underfitting Model', 'Overfitting Model', 'Final Model']\n",
        ")"
      ],
      "metadata": {
        "id": "HLtsNT1IlWTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
        "\n",
        "test_filepaths = test_generator.filepaths\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, idx in enumerate(misclassified_indices[:5]):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    img = tf.keras.preprocessing.image.load_img(test_filepaths[idx], target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Actual: {class_names[true_classes[idx]]}\\nPredicted: {class_names[predicted_classes[idx]]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4F0kd3HuleBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**According to GeeksforGeeks and enhanced by Gemini (2025)**:\n",
        "\n",
        "Why did the simple model underfit the data?\n",
        "The simple model underfit because it lacked the capacity to learn the complex patterns in the image data. With only one convolutional layer and a small number of filters (16), it couldn't extract enough meaningful features (like textures, shapes, and object parts) to distinguish between the six different classes. As a result, both its training and validation accuracy remained low, indicating it couldn't even perform well on the data it was trained on.\n",
        "\n",
        "Why did the complex model overfit?\n",
        "The complex model overfit because it had too much capacity and was trained for too long without any regularization. The deep architecture with many layers and neurons allowed it to essentially memorize the training data, including its noise and random fluctuations, rather than learning the general underlying patterns. This is visible in the training graphs where the training accuracy soared towards 100%, while the validation accuracy stagnated or even decreased, creating a large gap between the two curves.\n",
        "\n",
        "What role does Dropout play in preventing overfitting?\n",
        "Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of neuron activations to zero during each training update. This has two key effects:\n",
        "\n",
        "Reduces Co-adaptation: It prevents neurons from becoming overly reliant on each other. Since any neuron can be \"dropped out,\" the network is forced to learn more robust and redundant features.\n",
        "\n",
        "Ensemble Effect: It can be thought of as training a large number of different, smaller neural networks. The final predictions are an average from this \"ensemble,\" which generally leads to better generalization.\n",
        "\n",
        "How does the number of convolutional layers affect feature learning?\n",
        "The number of convolutional layers determines the hierarchy and complexity of features the model can learn.\n",
        "\n",
        "Shallow Networks (Few Layers): Learn simple, low-level features like edges, corners, and color gradients.\n",
        "\n",
        "Deep Networks (Many Layers): Build upon the features from previous layers to learn more complex, high-level features. For example, a first layer might detect edges, a second might combine them to find textures or simple shapes (like an eye or a wheel), and a third might combine those to identify objects (like a face or a car). More layers allow the model to understand the image in a more abstract and semantically rich way.\n",
        "\n",
        "How does the number of epochs impact model performance?\n",
        "An epoch is one complete pass through the entire training dataset. The number of epochs is a critical hyperparameter:\n",
        "\n",
        "Too Few Epochs: Leads to underfitting. The model doesn't get enough opportunities to learn from the data, and its performance will be poor on both training and test sets.\n",
        "\n",
        "Too Many Epochs: Can lead to overfitting. After the model has learned the general patterns, it may start memorizing the noise in the training data, causing its performance on unseen data (validation set) to degrade. This is why techniques like EarlyStopping are so valuable—they stop the training process when performance on the validation set stops improving.\n",
        "\n",
        "What are the benefits of a confusion matrix over just looking at accuracy?\n",
        "While accuracy gives a single, overall score of performance, a confusion matrix provides a much more detailed and insightful view. Its benefits include:\n",
        "\n",
        "Per-Class Performance: It shows how well the model is doing for each individual class, revealing if the model is biased or struggling with specific categories.\n",
        "\n",
        "Error Analysis: It details the types of errors being made. For example, we can see if the model frequently mistakes 'sea' for 'glacier' or 'street' for 'buildings'. This information is invaluable for model improvement.\n",
        "\n",
        "Identifying Class Imbalance Issues: If one class has many more samples than others, a high accuracy score can be misleading. A confusion matrix reveals if the model is simply predicting the majority class all the time.\n",
        "\n",
        "Calculating Advanced Metrics: It's the basis for calculating other important metrics like Precision, Recall, and F1-score for each class, which provide a more nuanced understanding of performance than accuracy alone."
      ],
      "metadata": {
        "id": "mcEYUwS7l4ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference list:\n",
        "\n",
        "GeeksforGeeks, 2023. Confusion matrix in machine learning. [online] Available at: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/ [Accessed 29 August 2025].\n",
        "\n",
        "GeeksforGeeks, 2024. Difference between batch, epoch, and iteration in neural network. [online] Available at: https://www.geeksforgeeks.org/difference-between-batch-epoch-and-iteration/ [Accessed 29 August 2025].\n",
        "\n",
        "GeeksforGeeks, 2024. Dropout in neural networks. [online] Available at: https://www.geeksforgeeks.org/dropout-in-neural-networks/ [Accessed 29 August 2025].\n",
        "\n",
        "GeeksforGeeks, 2024. Introduction to convolutional neural network. [online] Available at: https://www.geeksforgeeks.org/introduction-to-convolutional-neural-network/ [Accessed 29 August 2025].\n",
        "\n",
        "GeeksforGeeks, 2023. Underfitting and overfitting in machine learning. [online] Available at: https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/ [Accessed 29 August 2025]."
      ],
      "metadata": {
        "id": "_eQnlwLkn39M"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNxWCTAM+pxBtaatjNNLNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}